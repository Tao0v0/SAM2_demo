{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "title"
   },
   "source": [
    "# SAM-2 Video Interactive Segmentation Demo\n",
    "\n",
    "In this demo, we apply **SAM-2** to video frames to segment objects **interactively**.\n",
    "Changing the prompt over time shows how the model tracks objects and handles occlusions.\n",
    "\n",
    "**课堂要点**\n",
    "- 在第 0 帧用“点/框”初始化对象；\n",
    "- 在第 20 帧再给新的提示（修正/切换目标），观察遮挡后如何继续跟踪；\n",
    "- 输出整段视频的分割结果，并导出 GIF/MP4。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup"
   },
   "outputs": [],
   "source": [
    "#@title Install & Imports\n",
    "!pip -q install sam2 huggingface_hub opencv-python imageio matplotlib\n",
    "import os, math, glob, io, contextlib\n",
    "import numpy as np\n",
    "import cv2, imageio.v2 as imageio\n",
    "from PIL import Image\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Tuple\n",
    "from sam2.sam2_video_predictor import SAM2VideoPredictor\n",
    "print('Torch:', torch.__version__, '| CUDA available:', torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "video"
   },
   "outputs": [],
   "source": [
    "#@title Prepare a small public video and extract frames\n",
    "VIDEO_URL = \"https://github.com/opencv/opencv/blob/master/samples/data/vtest.avi?raw=true\"  # 行人视频\n",
    "VIDEO_PATH = \"/content/video.avi\"\n",
    "FRAME_DIR  = \"/content/frames\"\n",
    "os.makedirs(FRAME_DIR, exist_ok=True)\n",
    "\n",
    "!wget -q -O \"$VIDEO_PATH\" \"$VIDEO_URL\"\n",
    "cap = cv2.VideoCapture(VIDEO_PATH)\n",
    "w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = cap.get(cv2.CAP_PROP_FPS) or 10\n",
    "\n",
    "MAX_FRAMES = 80  # 课堂演示足够\n",
    "frames_rgb = []\n",
    "i = 0\n",
    "while True:\n",
    "    ok, frame_bgr = cap.read()\n",
    "    if not ok or i >= MAX_FRAMES:\n",
    "        break\n",
    "    frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n",
    "    frames_rgb.append(frame_rgb)\n",
    "    cv2.imwrite(f\"{FRAME_DIR}/{i:05d}.jpg\", cv2.cvtColor(frame_rgb, cv2.COLOR_RGB2BGR))\n",
    "    i += 1\n",
    "cap.release()\n",
    "len(frames_rgb), (w, h), fps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load"
   },
   "outputs": [],
   "source": [
    "#@title Load SAM-2 video predictor\n",
    "MODEL_ID = \"facebook/sam2-hiera-small\"  # 速度/效果均衡；可换 large / base / tiny\n",
    "predictor = SAM2VideoPredictor.from_pretrained(MODEL_ID)\n",
    "inference_state = predictor.init_state(video_path=FRAME_DIR)  # 用帧目录初始化\n",
    "print('SAM-2 state initialized with', MODEL_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "prompts"
   },
   "outputs": [],
   "source": [
    "#@title Define interactive prompts over time (points/boxes)\n",
    "H, W = frames_rgb[0].shape[:2]\n",
    "\n",
    "def make_point(x, y):\n",
    "    # points shape: (N, 2); labels: 1=positive, 0=negative\n",
    "    return np.array([[x, y]], dtype=np.float32), np.array([1], dtype=np.int32)\n",
    "\n",
    "# 以 vtest.avi 的左侧行人为例：第0帧点一下；第20帧再点一下做修正\n",
    "p1, l1 = make_point(int(W*0.30), int(H*0.65))\n",
    "p2, l2 = make_point(int(W*0.35), int(H*0.62))\n",
    "\n",
    "USER_PROMPTS = [\n",
    "    {\"frame_idx\": 0,  \"obj_id\": 1, \"points\": p1, \"labels\": l1, \"box\": None},\n",
    "    {\"frame_idx\": 20, \"obj_id\": 1, \"points\": p2, \"labels\": l2, \"box\": None},\n",
    "]\n",
    "\n",
    "# 如想用框提示：把 USE_BOX=True；并设置 box=np.array([x1,y1,x2,y2],dtype=np.float32)\n",
    "USE_BOX = False\n",
    "if USE_BOX:\n",
    "    USER_PROMPTS = [\n",
    "        {\"frame_idx\": 0,  \"obj_id\": 1, \"points\": None, \"labels\": None,\n",
    "         \"box\": np.array([int(W*0.22), int(H*0.50), int(W*0.42), int(H*0.90)], dtype=np.float32)},\n",
    "        {\"frame_idx\": 20, \"obj_id\": 1, \"points\": None, \"labels\": None,\n",
    "         \"box\": np.array([int(W*0.26), int(H*0.48), int(W*0.46), int(H*0.88)], dtype=np.float32)},\n",
    "    ]\n",
    "\n",
    "USER_PROMPTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "run"
   },
   "outputs": [],
   "source": [
    "#@title Run interaction + propagate across the video; export GIF/MP4\n",
    "import contextlib, numpy as np, torch, cv2, imageio\n",
    "from PIL import Image\n",
    "\n",
    "def overlay_mask(img_rgb: np.ndarray, mask: np.ndarray, alpha: float=0.5) -> np.ndarray:\n",
    "    \"\"\"把二值/布尔 mask 叠加到图像上显示。\"\"\"\n",
    "    color = np.array([30, 180, 255], dtype=np.uint8)  # 青色\n",
    "    overlay = img_rgb.copy()\n",
    "    overlay[mask > 0] = (0.5*overlay[mask > 0] + 0.5*color).astype(np.uint8)\n",
    "    return overlay\n",
    "\n",
    "def to_np_uint8_mask(x):\n",
    "    \"\"\"把 Tensor/np 数组的 mask/ logits 统一成 np.uint8 的 [H,W] 掩码。\"\"\"\n",
    "    if isinstance(x, torch.Tensor):\n",
    "        x = x.detach().to(\"cpu\")\n",
    "        # 有些接口返回的是 logits，阈值 0 即可；若已是 bool/0-1 也能兼容\n",
    "        x = (x > 0).to(torch.uint8).numpy()\n",
    "    else:  # numpy\n",
    "        x = (x > 0).astype(np.uint8)\n",
    "    # 可能是 [1,H,W]，取第 0 维；若已是 [H,W] 不变\n",
    "    if x.ndim == 3 and x.shape[0] == 1:\n",
    "        x = x[0]\n",
    "    return x  # [H,W] uint8\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# T4 不支持 bfloat16，自动选择 AMP 精度\n",
    "if device == \"cuda\":\n",
    "    major_cc, _ = torch.cuda.get_device_capability(0)\n",
    "    amp_dtype = torch.bfloat16 if major_cc >= 8 else torch.float16\n",
    "    amp = torch.autocast(\"cuda\", dtype=amp_dtype)\n",
    "else:\n",
    "    amp = contextlib.nullcontext()\n",
    "\n",
    "rendered = {}\n",
    "with torch.inference_mode(), amp:\n",
    "    # 逐条交互提示：在指定帧添加点/框，立即返回该帧分割\n",
    "    for step in USER_PROMPTS:\n",
    "        fidx = step[\"frame_idx\"]\n",
    "        if step[\"points\"] is not None:\n",
    "            frame_idx, obj_ids, masks = predictor.add_new_points_or_box(\n",
    "                inference_state,\n",
    "                frame_idx=fidx, obj_id=step[\"obj_id\"],\n",
    "                points=step[\"points\"], labels=step[\"labels\"],\n",
    "                normalize_coords=False,\n",
    "            )\n",
    "        else:\n",
    "            frame_idx, obj_ids, masks = predictor.add_new_points_or_box(\n",
    "                inference_state,\n",
    "                frame_idx=fidx, obj_id=step[\"obj_id\"],\n",
    "                box=step[\"box\"], normalize_coords=False,\n",
    "            )\n",
    "\n",
    "        # masks 可能是 [num_masks,H,W] 的 Tensor/np；统一到 uint8 numpy\n",
    "        m0 = to_np_uint8_mask(masks[0])\n",
    "        vis = overlay_mask(frames_rgb[fidx], m0)\n",
    "        rendered[f\"interact@{fidx:03d}.png\"] = vis\n",
    "\n",
    "    # 把当前交互态传播到整段视频\n",
    "    out_frames = []\n",
    "    for fidx, obj_ids, masks in predictor.propagate_in_video(inference_state):\n",
    "        # 多目标时 masks 形状可能是 [num_objs,H,W]；做个并集演示\n",
    "        if isinstance(masks, torch.Tensor):\n",
    "            mm = (masks > 0).any(dim=0)         # -> [H,W] Tensor\n",
    "            mm = to_np_uint8_mask(mm)           # -> [H,W] np.uint8\n",
    "        else:\n",
    "            mm = (masks > 0).any(axis=0).astype(np.uint8)\n",
    "        vis = overlay_mask(frames_rgb[fidx], mm)\n",
    "        out_frames.append(vis)\n",
    "\n",
    "print(\"Frames rendered:\", len(out_frames))\n",
    "\n",
    "# 导出 GIF 和 MP4\n",
    "GIF_PATH = \"/content/sam2_result.gif\"\n",
    "MP4_PATH = \"/content/sam2_result.mp4\"\n",
    "fps_safe = fps or 10\n",
    "\n",
    "imageio.mimsave(GIF_PATH, [Image.fromarray(f) for f in out_frames], duration=max(1/fps_safe, 0.05))\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "vw = cv2.VideoWriter(MP4_PATH, fourcc, fps_safe, (w, h))\n",
    "for f in out_frames:\n",
    "    vw.write(cv2.cvtColor(f, cv2.COLOR_RGB2BGR))\n",
    "vw.release()\n",
    "print(\"Saved:\", GIF_PATH, MP4_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "snapshots"
   },
   "outputs": [],
   "source": [
    "#@title Quick snapshots (see how prompts change results over time)\n",
    "def show_images(imgs: List[Tuple[str, np.ndarray]], cols=3, size=4):\n",
    "    rows = math.ceil(len(imgs)/cols)\n",
    "    plt.figure(figsize=(cols*size, rows*size))\n",
    "    for i, (title, im) in enumerate(imgs, 1):\n",
    "        plt.subplot(rows, cols, i)\n",
    "        plt.imshow(im)\n",
    "        plt.title(title)\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "snapshots = []\n",
    "for k in sorted(rendered.keys()):\n",
    "    snapshots.append((k, rendered[k]))\n",
    "snapshots.append((\"propagate:last\", out_frames[-1]))\n",
    "show_images(snapshots, cols=3, size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "download"
   },
   "outputs": [],
   "source": [
    "#@title Download results (for report/assignment)\n",
    "from IPython.display import FileLink, display\n",
    "display(FileLink(\"/content/sam2_result.gif\"))\n",
    "display(FileLink(\"/content/sam2_result.mp4\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "notes"
   },
   "source": [
    "## Notes\n",
    "- 可把 `MODEL_ID` 改为 `facebook/sam2-hiera-large` 以获得更高精度（速度更慢）。\n",
    "- 如要多目标，重复使用不同 `obj_id` 的提示即可；内部会维持各对象的状态并共同传播。\n",
    "- 如需换自己的视频：上传到 `/content/your.mp4`，把 `VIDEO_PATH` 改成你的路径，重新解帧运行即可。"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SAM-2 Video Interactive Segmentation — Cells 1–3\n",
        "\n",
        "This notebook contains the **first three cells** of the SAM-2 course demo:\n",
        "\n",
        "1. **Install dependencies** (SAM2 + tooling)\n",
        "2. **Prepare frames**: download a small example video (or use your own), downscale & subsample, extract frames into `frames/`\n",
        "3. **Batch demo**: load **`SAM2VideoPredictor`**, add a box/point prompt, propagate through the video, overlay masks, and export **`sam2_result.mp4`**\n",
        "\n",
        "> These cells are designed to run on Colab (GPU preferred, CPU also works). You can copy this notebook to GitHub as-is.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 1 — Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "!pip -q install \"git+https://github.com/facebookresearch/sam2.git\" \\\n",
        "                huggingface_hub opencv-python imageio[ffmpeg] matplotlib tqdm requests\n",
        "print(\"✅ Dependencies installed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 2 — Download video & extract frames (downscale + subsample)\n",
        "\n",
        "- Replace `VIDEO_URL` with your video link **or** upload your own video and set `VIDEO_MP4` accordingly.\n",
        "- Frames are written to `frames/0.jpg, 1.jpg, ...`.\n",
        "- Adjust `TARGET_LONG_EDGE`, `STEP`, `MAX_FRAMES` to control speed/memory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import os, shutil, cv2, requests, glob\n",
        "from pathlib import Path\n",
        "\n",
        "# ---- Source video ----\n",
        "VIDEO_URL = \"https://storage.openvinotoolkit.org/repositories/openvino_notebooks/data/data/video/Coco%20Walking%20in%20Berkeley.mp4\"\n",
        "VIDEO_MP4 = \"demo.mp4\"   # if you upload a file, set this to your filename\n",
        "\n",
        "# ---- Frame extraction params ----\n",
        "TARGET_LONG_EDGE = 640   # downscale long edge to this (helps avoid OOM)\n",
        "STEP = 2                 # take 1 frame every STEP frames (2–4 saves time)\n",
        "MAX_FRAMES = 200         # hard limit for speed\n",
        "\n",
        "FRAMES_DIR = Path(\"frames\")\n",
        "FRAMES_DIR.mkdir(exist_ok=True)\n",
        "for f in FRAMES_DIR.glob(\"*.jpg\"): f.unlink()\n",
        "\n",
        "# ---- Download video if not present ----\n",
        "if not Path(VIDEO_MP4).exists():\n",
        "    print(\"Downloading sample video…\")\n",
        "    r = requests.get(VIDEO_URL, stream=True, timeout=60)\n",
        "    r.raise_for_status()\n",
        "    with open(VIDEO_MP4, \"wb\") as f:\n",
        "        for chunk in r.iter_content(chunk_size=1<<20):\n",
        "            if chunk: f.write(chunk)\n",
        "print(\"Video:\", VIDEO_MP4, f\"({os.path.getsize(VIDEO_MP4)/1e6:.2f} MB)\")\n",
        "\n",
        "# ---- Extract frames ----\n",
        "cap = cv2.VideoCapture(VIDEO_MP4)\n",
        "assert cap.isOpened(), \"Cannot open video\"\n",
        "fps = cap.get(cv2.CAP_PROP_FPS) or 30\n",
        "idx = 0; out_idx = 0\n",
        "while True:\n",
        "    ret, frame = cap.read()\n",
        "    if not ret: break\n",
        "    if idx % STEP == 0 and out_idx < MAX_FRAMES:\n",
        "        h, w = frame.shape[:2]\n",
        "        long_edge = max(h, w)\n",
        "        scale = TARGET_LONG_EDGE / long_edge\n",
        "        if scale < 1.0:\n",
        "            frame = cv2.resize(frame, (int(w*scale), int(h*scale)), interpolation=cv2.INTER_AREA)\n",
        "        cv2.imwrite(str(FRAMES_DIR / f\"{out_idx}.jpg\"), frame)\n",
        "        out_idx += 1\n",
        "    idx += 1\n",
        "cap.release()\n",
        "\n",
        "first = cv2.imread(str(FRAMES_DIR/\"0.jpg\"))\n",
        "h0, w0 = first.shape[:2]\n",
        "print(f\"✅ Frames: {out_idx}  |  size≈{w0}x{h0}  |  fps≈{fps:.1f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 3 — Run SAM-2 on frames and export `sam2_result.mp4`\n",
        "\n",
        "This cell loads `SAM2VideoPredictor` with the **tiny** checkpoint, adds an initial **box** on frame 0, optionally a corrective **point** on a later frame, then **propagates** masks across the video and overlays them while streaming to an MP4. Adjust the box/point as needed for your video."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import os, cv2, numpy as np, torch\n",
        "from pathlib import Path\n",
        "from sam2.sam2_video_predictor import SAM2VideoPredictor\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "torch.set_grad_enabled(False)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", device)\n",
        "\n",
        "FRAMES_DIR = Path(\"frames\")\n",
        "frame_files = sorted(FRAMES_DIR.glob(\"*.jpg\"), key=lambda p:int(p.stem))\n",
        "assert len(frame_files)>0, \"No frames found. Run Cell 2 first.\"\n",
        "\n",
        "# Load predictor (tiny = memory friendly). Alternatives: 'facebook/sam2.1-hiera-tiny', 'facebook/sam2-hiera-small'\n",
        "MODEL_ID = \"facebook/sam2-hiera-tiny\"\n",
        "predictor = SAM2VideoPredictor.from_pretrained(MODEL_ID, device=device)\n",
        "print(\"Loaded:\", MODEL_ID)\n",
        "\n",
        "# Init video state\n",
        "state = predictor.init_state(video_path=str(FRAMES_DIR))\n",
        "img0 = cv2.imread(str(frame_files[0]))\n",
        "H0, W0 = img0.shape[:2]\n",
        "\n",
        "# --- Prompts: a robust box on frame 0, plus an optional positive point later ---\n",
        "obj_id = 1\n",
        "box0 = np.array([0.45*W0, 0.15*H0, 0.75*W0, 0.90*H0], np.float32)  # adjust to your video\n",
        "predictor.add_new_points_or_box(state, frame_idx=0, obj_id=obj_id, box=box0)\n",
        "\n",
        "fi_fix = min(12, len(frame_files)-1)\n",
        "pt_fix = np.array([[0.60*W0, 0.25*H0]], np.float32)\n",
        "lb_fix = np.array([1], np.int32)\n",
        "predictor.add_new_points_or_box(state, frame_idx=fi_fix, obj_id=obj_id, points=pt_fix, labels=lb_fix)\n",
        "\n",
        "# --- Stream propagation & export MP4 ---\n",
        "OUT_MP4 = \"sam2_result.mp4\"\n",
        "fps_out = 12  # output FPS\n",
        "fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
        "writer = cv2.VideoWriter(OUT_MP4, fourcc, fps_out, (W0, H0))\n",
        "print(\"Writing:\", OUT_MP4)\n",
        "\n",
        "next_write = 0\n",
        "pending = {}\n",
        "color = (34, 204, 136)  # RGB for overlay\n",
        "\n",
        "def overlay_mask(rgb, mask, color=(34,204,136), alpha=0.65):\n",
        "    if mask is None: return rgb\n",
        "    m = mask.astype(bool)\n",
        "    over = rgb.astype(np.float32)\n",
        "    over[m] = (1 - alpha) * over[m] + alpha * np.array(color, dtype=np.float32)\n",
        "    return over.astype(np.uint8)\n",
        "\n",
        "for fi, out_obj_ids, out_mask_logits in predictor.propagate_in_video(state):\n",
        "    rgb = cv2.cvtColor(cv2.imread(str(frame_files[fi])), cv2.COLOR_BGR2RGB)\n",
        "    mask = None\n",
        "    for k in range(len(out_obj_ids)):\n",
        "        if int(out_obj_ids[k]) == obj_id:\n",
        "            mk = (out_mask_logits[k] > 0).detach().cpu().numpy().squeeze().astype(np.uint8)\n",
        "            if mk.shape != rgb.shape[:2]:\n",
        "                mk = cv2.resize(mk, (rgb.shape[1], rgb.shape[0]), interpolation=cv2.INTER_NEAREST)\n",
        "            mask = mk; break\n",
        "    over = overlay_mask(rgb, mask, color=color, alpha=0.65)\n",
        "    pending[fi] = cv2.cvtColor(over, cv2.COLOR_RGB2BGR)\n",
        "    while next_write in pending:\n",
        "        writer.write(pending.pop(next_write)); next_write += 1\n",
        "\n",
        "for fi in sorted(pending):\n",
        "    writer.write(pending[fi])\n",
        "writer.release()\n",
        "print(\"✅ Saved:\", OUT_MP4)\n",
        "\n",
        "# --- Quick preview of a few frames ---\n",
        "import matplotlib.pyplot as plt\n",
        "cap = cv2.VideoCapture(OUT_MP4); frames=[]\n",
        "while True:\n",
        "    ret, f = cap.read()\n",
        "    if not ret: break\n",
        "    frames.append(cv2.cvtColor(f, cv2.COLOR_BGR2RGB))\n",
        "cap.release()\n",
        "sel = [0, min(12, len(frames)-1), max(0, len(frames)-3)]\n",
        "plt.figure(figsize=(12,4))\n",
        "for i,fi in enumerate(sel):\n",
        "    plt.subplot(1,3,i+1); plt.imshow(frames[fi]); plt.title(f\"Frame {fi}\"); plt.axis('off')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}